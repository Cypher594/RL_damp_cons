{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Cypher594/RL_damp_cons/blob/main/Design_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1c0c227",
      "metadata": {
        "id": "d1c0c227"
      },
      "outputs": [],
      "source": [
        "x=df.drop(['amplitude','damping constant'],axis='columns')\n",
        "y = df['amplitude','damping constant']\n",
        "# y2= df['damping constant']\n",
        "from sklearn.model_selection import train_test_split\n",
        "x_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ed75aff8",
      "metadata": {
        "id": "ed75aff8"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "import numpy as np\n",
        "dt_amp = DecisionTreeRegressor(random_state=42)\n",
        "dt_amp.fit(x_train, y_train[0])\n",
        "y_pred_smp = dt_amp.predict(x_test)\n",
        "dt_amp.score(y_pred_amp,y_test[0])\n",
        "\n",
        "#Apply grid search for hyper parameters => verma.\n",
        "\n",
        "# amp_model = keras.Sequential([\n",
        "#     keras.layers.Dense(3, input_shape=(6,),activation='relu'),\n",
        "#     keras.layers.Dense(2, activation='relu'),\n",
        "#     keras.layers.Dense(1, activation = 'relu')\n",
        "# ])\n",
        "\n",
        "# amp_model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "# amp_model.fit(x_train,y_train[0],epochs = 100)\n",
        "\n",
        "# dc_model = keras.Sequential([\n",
        "#     keras.layers.Dense(3, input_shape=(6,),activation='relu'),\n",
        "#     keras.layers.Dense(2, activation='relu'),\n",
        "#     keras.layers.Dense(1, activation = 'relu')\n",
        "# ])\n",
        "\n",
        "# dc_model.compile(optimizer = 'adam', loss='mean_squared_error', metrics=['mean_squared_error'])\n",
        "# dc_model.fit(x_train,y_train[1],epochs = 100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0acbff81",
      "metadata": {
        "id": "0acbff81"
      },
      "outputs": [],
      "source": [
        "# amp_model.evaluate(x_test,y_test[0])\n",
        "# dc_model.evaluate(x_test,y_test[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d5e5817d",
      "metadata": {
        "id": "d5e5817d"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "from gym import Env\n",
        "from gym import spaces\n",
        "import numpy as np\n",
        "\n",
        "class enviro(Env):\n",
        "    def __init__(self,sensitivity,penetration,frequency,phase):\n",
        "        # Observation space consists of T1, T2, D, A, sensitvity,penetration,frequency,phase\n",
        "        self.observation_space = spaces.Box(low=np.array([0,0, 1, 1,0,0,0,0]), high=np.array([60, 25,10, 10,10000,10000,100,1000]), dtype=np.float32)\n",
        "\n",
        "        # Action space consists of the change in T1 and T2\n",
        "        self.action_space = spaces.Box(low=np.array([-1, -1]), high=np.array([1, 1]), dtype=np.float32)\n",
        "\n",
        "        # Initialize state variables\n",
        "        self.T1 = 30\n",
        "        self.T2 = 5\n",
        "        self.episode_length = 50\n",
        "        self.sensitivity = sensitivity\n",
        "        self.penetration = penetration\n",
        "        self.frequency = frequency\n",
        "        self.phase = phase\n",
        "        self.prev_D = 0\n",
        "        self.prev_A = 0\n",
        "\n",
        "    def step(self, action):\n",
        "        # Update T1 and T2 based on action\n",
        "#         self.T1 = np.clip(self.T1 + action[0], 0, 1)\n",
        "        self.episode_length = self.episode_length - 1\n",
        "        self.T1 = self.T1 + action[0]\n",
        "        if(self.T1<0):\n",
        "            self.T1 = 0\n",
        "        elif(self.T1>60):\n",
        "            self.T1=60\n",
        "\n",
        "        self.T2 = self.T2 + action[1]\n",
        "        if(self.T2<0):\n",
        "            self.T2 = 0\n",
        "        elif(self.T2>25):\n",
        "            self.T2=25\n",
        "\n",
        "        ip = [self.sensitivity,self.penetration,self.frequency,self.phase,self.T1,self.T2]\n",
        "\n",
        "        # Calculate derived variables\n",
        "        # D = dc_model.predict(ip)\n",
        "        # A = amp_model.predict(ip)\n",
        "        D = (self.T1)*(self.T2)\n",
        "        A = (100-self.T1)*(100-self.T2)\n",
        "        # Reward is based on the difference between current and previous values of D and A\n",
        "        reward = (D - self.prev_D) - 5*(A - self.prev_A)\n",
        "\n",
        "        # Save current values of D and A for the next time step\n",
        "        self.prev_D = D\n",
        "        self.prev_A = A\n",
        "\n",
        "        if(self.episode_length<=0):\n",
        "            done = True\n",
        "        else:\n",
        "            done = False\n",
        "\n",
        "        # Information is an empty dictionary\n",
        "        info = {}\n",
        "\n",
        "        # Return the observation, reward, done, and information\n",
        "        return np.array([self.T1, self.T2, D, A,self.sensitivity,self.penetration,self.frequency,self.phase]), reward, done, info\n",
        "\n",
        "    def reset(self):\n",
        "        # Reset T1 and T2 to their initial values\n",
        "        self.T1 = 30\n",
        "        self.T2 = 5\n",
        "\n",
        "        # Calculate initial values of D and A\n",
        "        self.prev_D = 0\n",
        "        self.prev_A = 0\n",
        "\n",
        "        self.episode_length = 50\n",
        "\n",
        "        # Return the initial observation\n",
        "        return np.array([self.T1, self.T2, self.prev_D, self.prev_A, self.sensitivity, self.penetration, self.frequency, self.phase])\n",
        "\n",
        "    def state_display(self):\n",
        "        print(\"T1 =>\",self.T1,\"\\n\")\n",
        "        print(\"T2 =>\",self.T2,\"\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b95d23bf",
      "metadata": {
        "id": "b95d23bf",
        "outputId": "05ae7fba-935e-466e-a7cd-011d3bd24453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Episode:1 Score:-68.74852343819293\n",
            "Episode:2 Score:-325.772513548408\n",
            "Episode:3 Score:54.943047341144776\n",
            "Episode:4 Score:-150.006860131228\n",
            "Episode:5 Score:149.60004736639894\n",
            "Episode:6 Score:-974.9924600501384\n",
            "Episode:7 Score:336.19862847119043\n",
            "Episode:8 Score:465.8564933671187\n",
            "Episode:9 Score:-606.9689419576086\n",
            "Episode:10 Score:70.22345087887379\n"
          ]
        }
      ],
      "source": [
        "env = enviro()\n",
        "episodes = 10\n",
        "for episode in range(1,episodes+1):\n",
        "    score = 0\n",
        "    state = env.reset()\n",
        "    done = False\n",
        "    while not done:\n",
        "        action = env.action_space.sample()\n",
        "        n_state,reward,done,info = env.step(action)\n",
        "        score += reward\n",
        "    print('Episode:{} Score:{}'.format(episode,score))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a5a376d",
      "metadata": {
        "id": "5a5a376d",
        "outputId": "aa3d76d5-75ba-4ae8-eb6a-10bb6a4e6b05"
      },
      "outputs": [
        {
          "ename": "IndexError",
          "evalue": "index 30 is out of bounds for axis 0 with size 4",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "Input \u001b[1;32mIn [11]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     10\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[1;32m---> 12\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandom() \u001b[38;5;241m<\u001b[39m eps \u001b[38;5;129;01mor\u001b[39;00m np\u001b[38;5;241m.\u001b[39msum(\u001b[43mq_table\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     13\u001b[0m         action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandint(\u001b[38;5;241m0\u001b[39m, env\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mn)\n\u001b[0;32m     14\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
            "\u001b[1;31mIndexError\u001b[0m: index 30 is out of bounds for axis 0 with size 4"
          ]
        }
      ],
      "source": [
        "# q learning withhout library\n",
        "discount_factor = 0.95\n",
        "eps = 0.5\n",
        "eps_decay_factor = 0.999\n",
        "learning_rate = 0.8\n",
        "num_episodes = 500\n",
        "q_table = np.zeros([4,2])\n",
        "for i in range(num_episodes):\n",
        "    state = env.reset()\n",
        "    eps *= eps_decay_factor\n",
        "    done = False\n",
        "    while not done:\n",
        "        if np.random.random() < eps or np.sum(q_table[state, :]) == 0:\n",
        "            action = np.random.randint(0, 2)\n",
        "        else:\n",
        "            action = np.argmax(q_table[state, :])\n",
        "        new_state, reward, done, _ = env.step(action)\n",
        "        q_table[state, action] += reward + learning_rate *(discount_factor * np.max(q_table[new_state, :]) - q_table[state, action])\n",
        "        state = new_state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68d4210d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "id": "68d4210d",
        "outputId": "84a1485d-24c7-430a-fc74-7d33ef608c13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-fc74bb02aff5>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstable_baselines3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstable_baselines3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0minput_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0menviro\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDQN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'MlpPolicy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtensorboard_log\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'stable_baselines3'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import stable_baselines3\n",
        "from stable_baselines3 import DQN\n",
        "input_new = [0.3,0,1.2,6]\n",
        "env= enviro(input_new)\n",
        "model = DQN('MlpPolicy',env,verbose=1,tensorboard_log=log_path)\n",
        "model.learn(total_timesteps=20000)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd10cf0e",
      "metadata": {
        "id": "dd10cf0e",
        "outputId": "cb7fabea-d920-4a11-8d08-d619a962e7f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 51       |\n",
            "|    ep_rew_mean     | -63.9    |\n",
            "| time/              |          |\n",
            "|    fps             | 197      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 10       |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 159         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 231         |\n",
            "|    iterations           | 2           |\n",
            "|    time_elapsed         | 17          |\n",
            "|    total_timesteps      | 4096        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.021148909 |\n",
            "|    clip_fraction        | 0.294       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.83       |\n",
            "|    explained_variance   | 0.000244    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.02e+04    |\n",
            "|    n_updates            | 10          |\n",
            "|    policy_gradient_loss | -0.0431     |\n",
            "|    std                  | 0.989       |\n",
            "|    value_loss           | 4.31e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 348         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 249         |\n",
            "|    iterations           | 3           |\n",
            "|    time_elapsed         | 24          |\n",
            "|    total_timesteps      | 6144        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.018593732 |\n",
            "|    clip_fraction        | 0.221       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.8        |\n",
            "|    explained_variance   | 0.0164      |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 2.2e+04     |\n",
            "|    n_updates            | 20          |\n",
            "|    policy_gradient_loss | -0.0287     |\n",
            "|    std                  | 0.972       |\n",
            "|    value_loss           | 3.07e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 489         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 259         |\n",
            "|    iterations           | 4           |\n",
            "|    time_elapsed         | 31          |\n",
            "|    total_timesteps      | 8192        |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015261566 |\n",
            "|    clip_fraction        | 0.137       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.75       |\n",
            "|    explained_variance   | 0.145       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.15e+04    |\n",
            "|    n_updates            | 30          |\n",
            "|    policy_gradient_loss | -0.0159     |\n",
            "|    std                  | 0.947       |\n",
            "|    value_loss           | 2.4e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 526         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 265         |\n",
            "|    iterations           | 5           |\n",
            "|    time_elapsed         | 38          |\n",
            "|    total_timesteps      | 10240       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011326392 |\n",
            "|    clip_fraction        | 0.158       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.73       |\n",
            "|    explained_variance   | 0.213       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.93e+03    |\n",
            "|    n_updates            | 40          |\n",
            "|    policy_gradient_loss | -0.0131     |\n",
            "|    std                  | 0.947       |\n",
            "|    value_loss           | 1.72e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 51           |\n",
            "|    ep_rew_mean          | 540          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 269          |\n",
            "|    iterations           | 6            |\n",
            "|    time_elapsed         | 45           |\n",
            "|    total_timesteps      | 12288        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0072010765 |\n",
            "|    clip_fraction        | 0.0619       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -2.69        |\n",
            "|    explained_variance   | 0.279        |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15e+04     |\n",
            "|    n_updates            | 50           |\n",
            "|    policy_gradient_loss | -0.0062      |\n",
            "|    std                  | 0.922        |\n",
            "|    value_loss           | 1.59e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 555         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 271         |\n",
            "|    iterations           | 7           |\n",
            "|    time_elapsed         | 52          |\n",
            "|    total_timesteps      | 14336       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007548039 |\n",
            "|    clip_fraction        | 0.0962      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.64       |\n",
            "|    explained_variance   | 0.312       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.03e+03    |\n",
            "|    n_updates            | 60          |\n",
            "|    policy_gradient_loss | -0.0089     |\n",
            "|    std                  | 0.904       |\n",
            "|    value_loss           | 1.32e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 560         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 273         |\n",
            "|    iterations           | 8           |\n",
            "|    time_elapsed         | 59          |\n",
            "|    total_timesteps      | 16384       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006865615 |\n",
            "|    clip_fraction        | 0.0855      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.63       |\n",
            "|    explained_variance   | 0.323       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.31e+03    |\n",
            "|    n_updates            | 70          |\n",
            "|    policy_gradient_loss | -0.00376    |\n",
            "|    std                  | 0.907       |\n",
            "|    value_loss           | 1.38e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 578         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 275         |\n",
            "|    iterations           | 9           |\n",
            "|    time_elapsed         | 66          |\n",
            "|    total_timesteps      | 18432       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008402005 |\n",
            "|    clip_fraction        | 0.0586      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.61       |\n",
            "|    explained_variance   | 0.35        |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.31e+03    |\n",
            "|    n_updates            | 80          |\n",
            "|    policy_gradient_loss | -0.00533    |\n",
            "|    std                  | 0.891       |\n",
            "|    value_loss           | 1.2e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 593         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 277         |\n",
            "|    iterations           | 10          |\n",
            "|    time_elapsed         | 73          |\n",
            "|    total_timesteps      | 20480       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009091799 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.58       |\n",
            "|    explained_variance   | 0.366       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 4.33e+03    |\n",
            "|    n_updates            | 90          |\n",
            "|    policy_gradient_loss | -0.00583    |\n",
            "|    std                  | 0.88        |\n",
            "|    value_loss           | 1.2e+04     |\n",
            "-----------------------------------------\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 610         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 278         |\n",
            "|    iterations           | 11          |\n",
            "|    time_elapsed         | 80          |\n",
            "|    total_timesteps      | 22528       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010088727 |\n",
            "|    clip_fraction        | 0.104       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.56       |\n",
            "|    explained_variance   | 0.327       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 3.98e+03    |\n",
            "|    n_updates            | 100         |\n",
            "|    policy_gradient_loss | -0.00659    |\n",
            "|    std                  | 0.874       |\n",
            "|    value_loss           | 1.23e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 51          |\n",
            "|    ep_rew_mean          | 628         |\n",
            "| time/                   |             |\n",
            "|    fps                  | 280         |\n",
            "|    iterations           | 12          |\n",
            "|    time_elapsed         | 87          |\n",
            "|    total_timesteps      | 24576       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007700844 |\n",
            "|    clip_fraction        | 0.0839      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -2.55       |\n",
            "|    explained_variance   | 0.331       |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.05e+04    |\n",
            "|    n_updates            | 110         |\n",
            "|    policy_gradient_loss | -0.00428    |\n",
            "|    std                  | 0.871       |\n",
            "|    value_loss           | 1.26e+04    |\n",
            "-----------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 51        |\n",
            "|    ep_rew_mean          | 634       |\n",
            "| time/                   |           |\n",
            "|    fps                  | 280       |\n",
            "|    iterations           | 13        |\n",
            "|    time_elapsed         | 94        |\n",
            "|    total_timesteps      | 26624     |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0119437 |\n",
            "|    clip_fraction        | 0.114     |\n",
            "|    clip_range           | 0.2       |\n",
            "|    entropy_loss         | -2.55     |\n",
            "|    explained_variance   | 0.331     |\n",
            "|    learning_rate        | 0.0003    |\n",
            "|    loss                 | 5.88e+03  |\n",
            "|    n_updates            | 120       |\n",
            "|    policy_gradient_loss | -0.00599  |\n",
            "|    std                  | 0.879     |\n",
            "|    value_loss           | 1.26e+04  |\n",
            "---------------------------------------\n"
          ]
        }
      ],
      "source": [
        "from stable_baselines3 import PPO\n",
        "env= enviro(0.3,0,1.2,6)\n",
        "\n",
        "model = PPO('MlpPolicy',env,verbose=1)\n",
        "model.learn(total_timesteps=20000)          #20000 is a small number btw"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}